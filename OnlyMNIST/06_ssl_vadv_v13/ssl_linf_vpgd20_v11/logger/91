INFO: Running with the following configs:
	seed : 0
	device : cuda:0
	data_dir : ../../data/mnist_v13
	domain : mnist_orig
	img_size : 28
	batch_size : 100
	num_epochs : 100
	num_iters_per_epoch : 1000
	optim : sgd
	lr : 0.05
	momentum : 0.9
	l2_params : 0.0005
	max_grad_norm : 1
	scheduler_t0 : 20
	scheduler_tmult : 4
	vat_niters : 1
	vat_eps : 2.0
	vat_xi : 1e-06
	vat_consis_coef : 0.3
	lenet_depth : 28
	lenet_widen_factor : 1
	lenet_droprate : 0.3
	fc_nlayers : 1
	fc_hidden_dim : 1024
	fc_droprate : 0.3
	model_id : 10
	model_dir : ./saved_models
	eval_set : test_lbl
	ckpt_name : best_model.ckpt
	load : False

INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 0, it 251 >> 25.00 (18.966 sec) : loss = 0.62177
INFO: epoch 0, it 502 >> 50.10 (37.239 sec) : loss = 0.51383
INFO: epoch 0, it 753 >> 75.20 (55.456 sec) : loss = 0.50442
INFO: epoch 0, it 1000 >> 100.00 (72.608 sec) : lr 0.0500, train loss 0.66744
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 4.1
INFO: test : error = 4.4
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 1, it 1251 >> 25.00 (18.507 sec) : loss = 0.40757
INFO: epoch 1, it 1502 >> 50.10 (36.748 sec) : loss = 0.41170
INFO: epoch 1, it 1753 >> 75.20 (55.095 sec) : loss = 0.36438
INFO: epoch 1, it 2000 >> 100.00 (72.244 sec) : lr 0.0497, train loss 0.38963
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 4.18
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 2, it 2251 >> 25.00 (18.610 sec) : loss = 0.33973
INFO: epoch 2, it 2502 >> 50.10 (36.928 sec) : loss = 0.34165
INFO: epoch 2, it 2753 >> 75.20 (55.165 sec) : loss = 0.35828
INFO: epoch 2, it 3000 >> 100.00 (72.257 sec) : lr 0.0488, train loss 0.34653
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.0
INFO: test : error = 3.56
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 3, it 3251 >> 25.00 (18.562 sec) : loss = 0.36124
INFO: epoch 3, it 3502 >> 50.10 (36.813 sec) : loss = 0.33455
INFO: epoch 3, it 3753 >> 75.20 (55.113 sec) : loss = 0.29666
INFO: epoch 3, it 4000 >> 100.00 (72.187 sec) : lr 0.0473, train loss 0.32305
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.9
INFO: test : error = 3.49
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 4, it 4251 >> 25.00 (18.510 sec) : loss = 0.33067
INFO: epoch 4, it 4502 >> 50.10 (36.749 sec) : loss = 0.30587
INFO: epoch 4, it 4753 >> 75.20 (54.955 sec) : loss = 0.25908
INFO: epoch 4, it 5000 >> 100.00 (72.108 sec) : lr 0.0452, train loss 0.31217
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.4
INFO: test : error = 3.92
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 5, it 5251 >> 25.00 (18.543 sec) : loss = 0.25630
INFO: epoch 5, it 5502 >> 50.10 (36.809 sec) : loss = 0.28169
INFO: epoch 5, it 5753 >> 75.20 (55.062 sec) : loss = 0.35527
INFO: epoch 5, it 6000 >> 100.00 (72.168 sec) : lr 0.0427, train loss 0.28948
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 3.43
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 6, it 6251 >> 25.00 (18.567 sec) : loss = 0.30348
INFO: epoch 6, it 6502 >> 50.10 (36.934 sec) : loss = 0.22380
INFO: epoch 6, it 6753 >> 75.20 (55.271 sec) : loss = 0.26578
INFO: epoch 6, it 7000 >> 100.00 (72.383 sec) : lr 0.0397, train loss 0.27463
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 3.29
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 7, it 7251 >> 25.00 (18.605 sec) : loss = 0.26893
INFO: epoch 7, it 7502 >> 50.10 (36.888 sec) : loss = 0.31284
INFO: epoch 7, it 7753 >> 75.20 (55.117 sec) : loss = 0.23938
INFO: epoch 7, it 8000 >> 100.00 (72.250 sec) : lr 0.0363, train loss 0.26401
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 3.33
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 8, it 8251 >> 25.00 (18.533 sec) : loss = 0.23361
INFO: epoch 8, it 8502 >> 50.10 (36.780 sec) : loss = 0.26153
INFO: epoch 8, it 8753 >> 75.20 (55.059 sec) : loss = 0.24310
INFO: epoch 8, it 9000 >> 100.00 (72.145 sec) : lr 0.0327, train loss 0.25854
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 3.11
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 9, it 9251 >> 25.00 (18.530 sec) : loss = 0.20981
INFO: epoch 9, it 9502 >> 50.10 (36.841 sec) : loss = 0.29537
INFO: epoch 9, it 9753 >> 75.20 (55.155 sec) : loss = 0.24949
INFO: epoch 9, it 10000 >> 100.00 (72.289 sec) : lr 0.0289, train loss 0.25244
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.9
INFO: test : error = 3.4
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 10, it 10251 >> 25.00 (18.560 sec) : loss = 0.28220
INFO: epoch 10, it 10502 >> 50.10 (36.841 sec) : loss = 0.19278
INFO: epoch 10, it 10753 >> 75.20 (55.106 sec) : loss = 0.22574
INFO: epoch 10, it 11000 >> 100.00 (72.230 sec) : lr 0.0250, train loss 0.24681
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 3.29
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 11, it 11251 >> 25.00 (18.632 sec) : loss = 0.24955
INFO: epoch 11, it 11502 >> 50.10 (36.961 sec) : loss = 0.21266
INFO: epoch 11, it 11753 >> 75.20 (55.180 sec) : loss = 0.26942
INFO: epoch 11, it 12000 >> 100.00 (72.263 sec) : lr 0.0211, train loss 0.24244
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 3.0
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 12, it 12251 >> 25.00 (18.567 sec) : loss = 0.23351
INFO: epoch 12, it 12502 >> 50.10 (36.821 sec) : loss = 0.24258
INFO: epoch 12, it 12753 >> 75.20 (55.068 sec) : loss = 0.21310
INFO: epoch 12, it 13000 >> 100.00 (72.204 sec) : lr 0.0173, train loss 0.23625
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.6
INFO: test : error = 3.14
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 13, it 13251 >> 25.00 (18.610 sec) : loss = 0.23926
INFO: epoch 13, it 13502 >> 50.10 (36.954 sec) : loss = 0.22100
INFO: epoch 13, it 13753 >> 75.20 (55.247 sec) : loss = 0.23054
INFO: epoch 13, it 14000 >> 100.00 (72.387 sec) : lr 0.0137, train loss 0.23350
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 3.19
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 14, it 14251 >> 25.00 (18.553 sec) : loss = 0.23224
INFO: epoch 14, it 14502 >> 50.10 (36.865 sec) : loss = 0.20347
INFO: epoch 14, it 14753 >> 75.20 (55.143 sec) : loss = 0.22023
INFO: epoch 14, it 15000 >> 100.00 (72.253 sec) : lr 0.0103, train loss 0.22715
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 3.06
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 15, it 15251 >> 25.00 (18.492 sec) : loss = 0.21385
INFO: epoch 15, it 15502 >> 50.10 (36.745 sec) : loss = 0.18793
INFO: epoch 15, it 15753 >> 75.20 (54.956 sec) : loss = 0.22593
INFO: epoch 15, it 16000 >> 100.00 (72.099 sec) : lr 0.0073, train loss 0.22322
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 2.99
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 16, it 16251 >> 25.00 (18.611 sec) : loss = 0.22653
INFO: epoch 16, it 16502 >> 50.10 (36.875 sec) : loss = 0.22239
INFO: epoch 16, it 16753 >> 75.20 (55.120 sec) : loss = 0.25932
INFO: epoch 16, it 17000 >> 100.00 (72.204 sec) : lr 0.0048, train loss 0.21856
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 2.91
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 17, it 17251 >> 25.00 (18.486 sec) : loss = 0.22823
INFO: epoch 17, it 17502 >> 50.10 (36.824 sec) : loss = 0.20538
INFO: epoch 17, it 17753 >> 75.20 (55.113 sec) : loss = 0.26200
INFO: epoch 17, it 18000 >> 100.00 (72.203 sec) : lr 0.0027, train loss 0.21585
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 2.89
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 18, it 18251 >> 25.00 (18.557 sec) : loss = 0.23661
INFO: epoch 18, it 18502 >> 50.10 (36.821 sec) : loss = 0.24383
INFO: epoch 18, it 18753 >> 75.20 (55.082 sec) : loss = 0.18400
INFO: epoch 18, it 19000 >> 100.00 (72.207 sec) : lr 0.0012, train loss 0.21363
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 2.99
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 19, it 19251 >> 25.00 (18.694 sec) : loss = 0.23051
INFO: epoch 19, it 19502 >> 50.10 (36.951 sec) : loss = 0.19340
INFO: epoch 19, it 19753 >> 75.20 (55.174 sec) : loss = 0.21664
INFO: epoch 19, it 20000 >> 100.00 (72.282 sec) : lr 0.0003, train loss 0.21502
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 2.94
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 20, it 20251 >> 25.00 (18.555 sec) : loss = 0.27370
INFO: epoch 20, it 20502 >> 50.10 (36.811 sec) : loss = 0.24672
INFO: epoch 20, it 20753 >> 75.20 (55.108 sec) : loss = 0.25451
INFO: epoch 20, it 21000 >> 100.00 (72.253 sec) : lr 0.0500, train loss 0.27567
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.5
INFO: test : error = 3.97
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 21, it 21251 >> 25.00 (18.511 sec) : loss = 0.26058
INFO: epoch 21, it 21502 >> 50.10 (36.752 sec) : loss = 0.22746
INFO: epoch 21, it 21753 >> 75.20 (55.013 sec) : loss = 0.27657
INFO: epoch 21, it 22000 >> 100.00 (72.115 sec) : lr 0.0500, train loss 0.26900
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 3.27
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 22, it 22251 >> 25.00 (18.613 sec) : loss = 0.23169
INFO: epoch 22, it 22502 >> 50.10 (36.865 sec) : loss = 0.24189
INFO: epoch 22, it 22753 >> 75.20 (55.101 sec) : loss = 0.26395
INFO: epoch 22, it 23000 >> 100.00 (72.694 sec) : lr 0.0499, train loss 0.26412
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.9
INFO: test : error = 3.5
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 23, it 23251 >> 25.00 (18.674 sec) : loss = 0.19838
INFO: epoch 23, it 23502 >> 50.10 (37.035 sec) : loss = 0.26033
INFO: epoch 23, it 23753 >> 75.20 (55.328 sec) : loss = 0.25900
INFO: epoch 23, it 24000 >> 100.00 (72.563 sec) : lr 0.0498, train loss 0.25528
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.5
INFO: test : error = 3.51
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 24, it 24251 >> 25.00 (18.546 sec) : loss = 0.27906
INFO: epoch 24, it 24502 >> 50.10 (36.824 sec) : loss = 0.28277
INFO: epoch 24, it 24753 >> 75.20 (55.073 sec) : loss = 0.23868
INFO: Running with the following configs:
	seed : 0
	device : cuda:0
	data_dir : ../../data/mnist_v13
	domain : mnist_orig
	img_size : 28
	batch_size : 100
	num_epochs : 100
	num_iters_per_epoch : 1000
	optim : sgd
	lr : 0.05
	momentum : 0.9
	l2_params : 0.0005
	max_grad_norm : 1
	scheduler_t0 : 20
	scheduler_tmult : 4
	vat_niters : 1
	vat_eps : 2.0
	vat_xi : 1e-06
	vat_consis_coef : 0.3
	lenet_depth : 28
	lenet_widen_factor : 1
	lenet_droprate : 0.3
	fc_nlayers : 1
	fc_hidden_dim : 1024
	fc_droprate : 0.3
	model_id : 10
	model_dir : ./saved_models
	eval_set : test_lbl
	ckpt_name : best_model.ckpt
	load : False

INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 0, it 251 >> 25.00 (21.847 sec) : loss = 0.79819
INFO: epoch 0, it 502 >> 50.10 (43.039 sec) : loss = 0.51567
INFO: epoch 0, it 753 >> 75.20 (64.319 sec) : loss = 0.45029
INFO: epoch 0, it 1000 >> 100.00 (84.445 sec) : lr 0.0500, train loss 0.75365
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 5.1
INFO: test : error = 4.47
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 1, it 1251 >> 25.00 (21.552 sec) : loss = 0.34482
INFO: epoch 1, it 1502 >> 50.10 (42.824 sec) : loss = 0.34731
INFO: epoch 1, it 1753 >> 75.20 (64.227 sec) : loss = 0.33967
INFO: epoch 1, it 2000 >> 100.00 (84.431 sec) : lr 0.0497, train loss 0.35521
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 4.6
INFO: test : error = 3.99
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 2, it 2251 >> 25.00 (21.574 sec) : loss = 0.27269
INFO: epoch 2, it 2502 >> 50.10 (42.803 sec) : loss = 0.30888
INFO: epoch 2, it 2753 >> 75.20 (64.120 sec) : loss = 0.30236
INFO: epoch 2, it 3000 >> 100.00 (84.179 sec) : lr 0.0488, train loss 0.29717
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.3
INFO: test : error = 3.55
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 3, it 3251 >> 25.00 (21.512 sec) : loss = 0.21561
INFO: epoch 3, it 3502 >> 50.10 (42.933 sec) : loss = 0.27623
INFO: epoch 3, it 3753 >> 75.20 (64.249 sec) : loss = 0.27813
INFO: epoch 3, it 4000 >> 100.00 (84.361 sec) : lr 0.0473, train loss 0.27148
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.4
INFO: test : error = 3.28
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 4, it 4251 >> 25.00 (21.600 sec) : loss = 0.31267
INFO: epoch 4, it 4502 >> 50.10 (42.926 sec) : loss = 0.26649
INFO: epoch 4, it 4753 >> 75.20 (64.275 sec) : loss = 0.21481
INFO: epoch 4, it 5000 >> 100.00 (84.493 sec) : lr 0.0452, train loss 0.26069
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 4.1
INFO: test : error = 4.05
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 5, it 5251 >> 25.00 (21.640 sec) : loss = 0.24443
INFO: epoch 5, it 5502 >> 50.10 (43.001 sec) : loss = 0.20123
INFO: epoch 5, it 5753 >> 75.20 (64.269 sec) : loss = 0.28468
INFO: epoch 5, it 6000 >> 100.00 (84.424 sec) : lr 0.0427, train loss 0.25190
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.9
INFO: test : error = 3.25
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 6, it 6251 >> 25.00 (22.157 sec) : loss = 0.25720
INFO: epoch 6, it 6502 >> 50.10 (43.644 sec) : loss = 0.23581
INFO: epoch 6, it 6753 >> 75.20 (64.916 sec) : loss = 0.24541
INFO: epoch 6, it 7000 >> 100.00 (85.042 sec) : lr 0.0397, train loss 0.24229
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.8
INFO: test : error = 3.77
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 7, it 7251 >> 25.00 (21.539 sec) : loss = 0.22423
INFO: epoch 7, it 7502 >> 50.10 (42.910 sec) : loss = 0.20162
INFO: epoch 7, it 7753 >> 75.20 (64.334 sec) : loss = 0.25822
INFO: epoch 7, it 8000 >> 100.00 (84.604 sec) : lr 0.0363, train loss 0.23323
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.3
INFO: test : error = 3.01
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 8, it 8251 >> 25.00 (21.650 sec) : loss = 0.20497
INFO: epoch 8, it 8502 >> 50.10 (42.983 sec) : loss = 0.23530
INFO: epoch 8, it 8753 >> 75.20 (64.363 sec) : loss = 0.20166
INFO: epoch 8, it 9000 >> 100.00 (84.579 sec) : lr 0.0327, train loss 0.22607
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 3.12
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 9, it 9251 >> 25.00 (21.561 sec) : loss = 0.21995
INFO: epoch 9, it 9502 >> 50.10 (42.978 sec) : loss = 0.23017
INFO: epoch 9, it 9753 >> 75.20 (64.348 sec) : loss = 0.26107
INFO: epoch 9, it 10000 >> 100.00 (84.416 sec) : lr 0.0289, train loss 0.21963
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.1
INFO: test : error = 3.1
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 10, it 10251 >> 25.00 (21.611 sec) : loss = 0.22663
INFO: epoch 10, it 10502 >> 50.10 (43.006 sec) : loss = 0.16683
INFO: epoch 10, it 10753 >> 75.20 (64.325 sec) : loss = 0.19427
INFO: epoch 10, it 11000 >> 100.00 (84.307 sec) : lr 0.0250, train loss 0.20972
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.4
INFO: test : error = 3.42
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 11, it 11251 >> 25.00 (21.732 sec) : loss = 0.24964
INFO: epoch 11, it 11502 >> 50.10 (43.245 sec) : loss = 0.16024
INFO: epoch 11, it 11753 >> 75.20 (64.566 sec) : loss = 0.20400
INFO: epoch 11, it 12000 >> 100.00 (84.699 sec) : lr 0.0211, train loss 0.20631
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.5
INFO: test : error = 2.93
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 12, it 12251 >> 25.00 (21.753 sec) : loss = 0.18571
INFO: epoch 12, it 12502 >> 50.10 (43.097 sec) : loss = 0.22345
INFO: epoch 12, it 12753 >> 75.20 (64.433 sec) : loss = 0.24075
INFO: epoch 12, it 13000 >> 100.00 (84.580 sec) : lr 0.0173, train loss 0.19568
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 2.97
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 13, it 13251 >> 25.00 (21.635 sec) : loss = 0.26057
INFO: epoch 13, it 13502 >> 50.10 (43.604 sec) : loss = 0.19123
INFO: epoch 13, it 13753 >> 75.20 (65.037 sec) : loss = 0.16520
INFO: epoch 13, it 14000 >> 100.00 (85.121 sec) : lr 0.0137, train loss 0.19031
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 2.85
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 14, it 14251 >> 25.00 (21.719 sec) : loss = 0.14097
INFO: epoch 14, it 14502 >> 50.10 (43.242 sec) : loss = 0.18845
INFO: epoch 14, it 14753 >> 75.20 (64.567 sec) : loss = 0.15411
INFO: epoch 14, it 15000 >> 100.00 (84.620 sec) : lr 0.0103, train loss 0.18076
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.0
INFO: test : error = 2.95
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 15, it 15251 >> 25.00 (21.698 sec) : loss = 0.15983
INFO: epoch 15, it 15502 >> 50.10 (43.039 sec) : loss = 0.15950
INFO: epoch 15, it 15753 >> 75.20 (64.468 sec) : loss = 0.16331
INFO: epoch 15, it 16000 >> 100.00 (84.558 sec) : lr 0.0073, train loss 0.17357
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 2.91
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 16, it 16251 >> 25.00 (21.601 sec) : loss = 0.17428
INFO: epoch 16, it 16502 >> 50.10 (42.922 sec) : loss = 0.22425
INFO: epoch 16, it 16753 >> 75.20 (64.235 sec) : loss = 0.16930
INFO: epoch 16, it 17000 >> 100.00 (84.371 sec) : lr 0.0048, train loss 0.16773
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.0
INFO: test : error = 3.01
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 17, it 17251 >> 25.00 (21.731 sec) : loss = 0.16897
INFO: epoch 17, it 17502 >> 50.10 (43.125 sec) : loss = 0.13869
INFO: epoch 17, it 17753 >> 75.20 (64.427 sec) : loss = 0.19965
INFO: epoch 17, it 18000 >> 100.00 (84.553 sec) : lr 0.0027, train loss 0.16186
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 2.58
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 18, it 18251 >> 25.00 (21.712 sec) : loss = 0.16363
INFO: epoch 18, it 18502 >> 50.10 (43.095 sec) : loss = 0.17225
INFO: epoch 18, it 18753 >> 75.20 (64.380 sec) : loss = 0.13649
INFO: epoch 18, it 19000 >> 100.00 (84.439 sec) : lr 0.0012, train loss 0.15792
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.1
INFO: test : error = 2.71
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 19, it 19251 >> 25.00 (21.731 sec) : loss = 0.14818
INFO: epoch 19, it 19502 >> 50.10 (43.078 sec) : loss = 0.13423
INFO: epoch 19, it 19753 >> 75.20 (64.355 sec) : loss = 0.16413
INFO: epoch 19, it 20000 >> 100.00 (84.457 sec) : lr 0.0003, train loss 0.15624
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 2.64
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 20, it 20251 >> 25.00 (21.816 sec) : loss = 0.28833
INFO: epoch 20, it 20502 >> 50.10 (43.110 sec) : loss = 0.26962
INFO: epoch 20, it 20753 >> 75.20 (64.365 sec) : loss = 0.28063
INFO: epoch 20, it 21000 >> 100.00 (84.528 sec) : lr 0.0500, train loss 0.26260
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.0
INFO: test : error = 3.24
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 21, it 21251 >> 25.00 (21.616 sec) : loss = 0.22136
INFO: epoch 21, it 21502 >> 50.10 (42.972 sec) : loss = 0.19116
INFO: epoch 21, it 21753 >> 75.20 (64.391 sec) : loss = 0.24076
INFO: epoch 21, it 22000 >> 100.00 (84.511 sec) : lr 0.0500, train loss 0.24169
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.4
INFO: test : error = 3.05
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 22, it 22251 >> 25.00 (21.561 sec) : loss = 0.22478
INFO: epoch 22, it 22502 >> 50.10 (42.888 sec) : loss = 0.23995
INFO: epoch 22, it 22753 >> 75.20 (64.168 sec) : loss = 0.24136
INFO: epoch 22, it 23000 >> 100.00 (84.282 sec) : lr 0.0499, train loss 0.23826
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 2.96
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 23, it 23251 >> 25.00 (21.636 sec) : loss = 0.19978
INFO: epoch 23, it 23502 >> 50.10 (43.107 sec) : loss = 0.19224
INFO: epoch 23, it 23753 >> 75.20 (64.469 sec) : loss = 0.26584
INFO: epoch 23, it 24000 >> 100.00 (84.616 sec) : lr 0.0498, train loss 0.23574
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.8
INFO: test : error = 3.17
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 24, it 24251 >> 25.00 (21.691 sec) : loss = 0.30123
INFO: epoch 24, it 24502 >> 50.10 (43.193 sec) : loss = 0.23553
INFO: epoch 24, it 24753 >> 75.20 (64.521 sec) : loss = 0.23657
INFO: epoch 24, it 25000 >> 100.00 (84.698 sec) : lr 0.0497, train loss 0.23263
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.1
INFO: test : error = 2.99
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 25, it 25251 >> 25.00 (21.630 sec) : loss = 0.20576
INFO: epoch 25, it 25502 >> 50.10 (42.937 sec) : loss = 0.23026
INFO: epoch 25, it 25753 >> 75.20 (64.184 sec) : loss = 0.22922
INFO: epoch 25, it 26000 >> 100.00 (84.257 sec) : lr 0.0495, train loss 0.23443
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.2
INFO: test : error = 3.41
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 26, it 26251 >> 25.00 (21.687 sec) : loss = 0.25136
INFO: epoch 26, it 26502 >> 50.10 (43.062 sec) : loss = 0.21197
INFO: epoch 26, it 26753 >> 75.20 (64.430 sec) : loss = 0.22858
INFO: epoch 26, it 27000 >> 100.00 (84.483 sec) : lr 0.0493, train loss 0.23206
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.9
INFO: test : error = 3.25
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 27, it 27251 >> 25.00 (21.624 sec) : loss = 0.24209
INFO: epoch 27, it 27502 >> 50.10 (42.930 sec) : loss = 0.23796
INFO: epoch 27, it 27753 >> 75.20 (64.335 sec) : loss = 0.27137
INFO: epoch 27, it 28000 >> 100.00 (84.472 sec) : lr 0.0491, train loss 0.23105
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.9
INFO: test : error = 3.21
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 28, it 28251 >> 25.00 (21.612 sec) : loss = 0.18797
INFO: epoch 28, it 28502 >> 50.10 (42.903 sec) : loss = 0.25774
INFO: epoch 28, it 28753 >> 75.20 (64.227 sec) : loss = 0.26729
INFO: epoch 28, it 29000 >> 100.00 (84.429 sec) : lr 0.0488, train loss 0.23060
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.5
INFO: test : error = 3.04
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 29, it 29251 >> 25.00 (21.651 sec) : loss = 0.20459
INFO: epoch 29, it 29502 >> 50.10 (43.050 sec) : loss = 0.21254
INFO: epoch 29, it 29753 >> 75.20 (64.451 sec) : loss = 0.23902
INFO: epoch 29, it 30000 >> 100.00 (84.558 sec) : lr 0.0485, train loss 0.23177
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.4
INFO: test : error = 3.21
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 30, it 30251 >> 25.00 (21.553 sec) : loss = 0.20945
INFO: epoch 30, it 30502 >> 50.10 (42.984 sec) : loss = 0.22477
INFO: epoch 30, it 30753 >> 75.20 (64.325 sec) : loss = 0.21614
INFO: epoch 30, it 31000 >> 100.00 (84.469 sec) : lr 0.0481, train loss 0.22898
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.3
INFO: test : error = 3.06
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 31, it 31251 >> 25.00 (21.618 sec) : loss = 0.25733
INFO: epoch 31, it 31502 >> 50.10 (42.978 sec) : loss = 0.23935
INFO: epoch 31, it 31753 >> 75.20 (64.374 sec) : loss = 0.20188
INFO: epoch 31, it 32000 >> 100.00 (84.451 sec) : lr 0.0477, train loss 0.24615
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 3.0
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 32, it 32251 >> 25.00 (21.619 sec) : loss = 0.25705
INFO: epoch 32, it 32502 >> 50.10 (42.913 sec) : loss = 0.35008
INFO: epoch 32, it 32753 >> 75.20 (64.241 sec) : loss = 0.22021
INFO: epoch 32, it 33000 >> 100.00 (84.293 sec) : lr 0.0473, train loss 0.25440
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.1
INFO: test : error = 3.01
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 33, it 33251 >> 25.00 (21.714 sec) : loss = 0.26703
INFO: epoch 33, it 33502 >> 50.10 (43.120 sec) : loss = 0.24583
INFO: epoch 33, it 33753 >> 75.20 (64.438 sec) : loss = 0.26577
INFO: epoch 33, it 34000 >> 100.00 (84.587 sec) : lr 0.0468, train loss 0.26222
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.1
INFO: test : error = 3.14
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 34, it 34251 >> 25.00 (21.594 sec) : loss = 0.31160
INFO: epoch 34, it 34502 >> 50.10 (42.955 sec) : loss = 0.23506
INFO: epoch 34, it 34753 >> 75.20 (64.438 sec) : loss = 0.26671
INFO: epoch 34, it 35000 >> 100.00 (84.578 sec) : lr 0.0463, train loss 0.26604
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 3.3
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 35, it 35251 >> 25.00 (21.510 sec) : loss = 0.23955
INFO: epoch 35, it 35502 >> 50.10 (42.851 sec) : loss = 0.27373
INFO: epoch 35, it 35753 >> 75.20 (64.278 sec) : loss = 0.28880
INFO: epoch 35, it 36000 >> 100.00 (84.315 sec) : lr 0.0458, train loss 0.26491
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.1
INFO: test : error = 3.13
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 36, it 36251 >> 25.00 (21.580 sec) : loss = 0.25154
INFO: epoch 36, it 36502 >> 50.10 (42.943 sec) : loss = 0.34946
INFO: epoch 36, it 36753 >> 75.20 (64.228 sec) : loss = 0.32096
INFO: epoch 36, it 37000 >> 100.00 (84.282 sec) : lr 0.0452, train loss 0.26733
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.0
INFO: test : error = 2.81
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 37, it 37251 >> 25.00 (21.606 sec) : loss = 0.24111
INFO: epoch 37, it 37502 >> 50.10 (42.950 sec) : loss = 0.22104
INFO: epoch 37, it 37753 >> 75.20 (64.223 sec) : loss = 0.25458
INFO: epoch 37, it 38000 >> 100.00 (84.283 sec) : lr 0.0446, train loss 0.26899
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 2.8
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 38, it 38251 >> 25.00 (21.548 sec) : loss = 0.41905
INFO: epoch 38, it 38502 >> 50.10 (42.923 sec) : loss = 0.29360
INFO: epoch 38, it 38753 >> 75.20 (64.236 sec) : loss = 0.28857
INFO: epoch 38, it 39000 >> 100.00 (84.250 sec) : lr 0.0440, train loss 0.27006
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 2.92
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 39, it 39251 >> 25.00 (21.713 sec) : loss = 0.24703
INFO: epoch 39, it 39502 >> 50.10 (43.118 sec) : loss = 0.21558
INFO: epoch 39, it 39753 >> 75.20 (64.535 sec) : loss = 0.24682
INFO: epoch 39, it 40000 >> 100.00 (84.658 sec) : lr 0.0434, train loss 0.26365
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 2.89
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 40, it 40251 >> 25.00 (21.726 sec) : loss = 0.50994
INFO: epoch 40, it 40502 >> 50.10 (43.154 sec) : loss = 0.37363
INFO: epoch 40, it 40753 >> 75.20 (64.527 sec) : loss = 0.24655
INFO: epoch 40, it 41000 >> 100.00 (84.598 sec) : lr 0.0427, train loss 0.26808
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.0
INFO: test : error = 3.52
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 41, it 41251 >> 25.00 (21.565 sec) : loss = 0.25646
INFO: epoch 41, it 41502 >> 50.10 (43.406 sec) : loss = 0.24075
INFO: epoch 41, it 41753 >> 75.20 (64.880 sec) : loss = 0.21123
INFO: epoch 41, it 42000 >> 100.00 (84.925 sec) : lr 0.0420, train loss 0.27420
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 3.48
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 42, it 42251 >> 25.00 (21.627 sec) : loss = 0.26095
INFO: epoch 42, it 42502 >> 50.10 (42.884 sec) : loss = 0.25626
INFO: epoch 42, it 42753 >> 75.20 (64.281 sec) : loss = 0.29649
INFO: epoch 42, it 43000 >> 100.00 (84.340 sec) : lr 0.0412, train loss 0.26544
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 2.84
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 43, it 43251 >> 25.00 (21.746 sec) : loss = 0.22430
INFO: epoch 43, it 43502 >> 50.10 (43.125 sec) : loss = 0.21879
INFO: epoch 43, it 43753 >> 75.20 (64.539 sec) : loss = 0.27352
INFO: epoch 43, it 44000 >> 100.00 (84.671 sec) : lr 0.0405, train loss 0.26545
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 3.12
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 44, it 44251 >> 25.00 (21.763 sec) : loss = 0.23186
INFO: epoch 44, it 44502 >> 50.10 (43.570 sec) : loss = 0.25581
INFO: epoch 44, it 44753 >> 75.20 (65.454 sec) : loss = 0.23990
INFO: epoch 44, it 45000 >> 100.00 (85.736 sec) : lr 0.0397, train loss 0.27165
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 3.01
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 45, it 45251 >> 25.00 (21.669 sec) : loss = 0.24382
INFO: epoch 45, it 45502 >> 50.10 (42.987 sec) : loss = 0.30101
INFO: epoch 45, it 45753 >> 75.20 (64.361 sec) : loss = 0.23519
INFO: epoch 45, it 46000 >> 100.00 (84.523 sec) : lr 0.0389, train loss 0.26757
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 3.17
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 46, it 46251 >> 25.00 (21.672 sec) : loss = 0.23395
INFO: epoch 46, it 46502 >> 50.10 (42.992 sec) : loss = 0.25496
INFO: epoch 46, it 46753 >> 75.20 (64.294 sec) : loss = 0.23739
INFO: epoch 46, it 47000 >> 100.00 (84.441 sec) : lr 0.0381, train loss 0.26590
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 3.0
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 47, it 47251 >> 25.00 (21.538 sec) : loss = 0.39890
INFO: epoch 47, it 47502 >> 50.10 (42.808 sec) : loss = 0.25804
INFO: epoch 47, it 47753 >> 75.20 (64.049 sec) : loss = 0.22331
INFO: epoch 47, it 48000 >> 100.00 (84.203 sec) : lr 0.0372, train loss 0.26920
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 3.07
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 48, it 48251 >> 25.00 (21.705 sec) : loss = 0.24253
INFO: epoch 48, it 48502 >> 50.10 (43.017 sec) : loss = 0.23776
INFO: epoch 48, it 48753 >> 75.20 (64.381 sec) : loss = 0.26955
INFO: epoch 48, it 49000 >> 100.00 (84.447 sec) : lr 0.0363, train loss 0.26611
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 3.05
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 49, it 49251 >> 25.00 (21.563 sec) : loss = 0.25579
INFO: epoch 49, it 49502 >> 50.10 (42.828 sec) : loss = 0.24900
INFO: epoch 49, it 49753 >> 75.20 (64.158 sec) : loss = 0.24123
INFO: epoch 49, it 50000 >> 100.00 (84.338 sec) : lr 0.0355, train loss 0.26289
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 3.23
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 50, it 50251 >> 25.00 (21.614 sec) : loss = 0.23666
INFO: epoch 50, it 50502 >> 50.10 (42.981 sec) : loss = 0.29772
INFO: epoch 50, it 50753 >> 75.20 (64.345 sec) : loss = 0.50126
INFO: epoch 50, it 51000 >> 100.00 (84.491 sec) : lr 0.0346, train loss 0.26754
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.8
INFO: test : error = 2.98
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 51, it 51251 >> 25.00 (21.534 sec) : loss = 0.22415
INFO: epoch 51, it 51502 >> 50.10 (42.872 sec) : loss = 0.28256
INFO: epoch 51, it 51753 >> 75.20 (64.149 sec) : loss = 0.26594
INFO: epoch 51, it 52000 >> 100.00 (84.328 sec) : lr 0.0337, train loss 0.26357
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 3.14
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 52, it 52251 >> 25.00 (21.676 sec) : loss = 0.23123
INFO: epoch 52, it 52502 >> 50.10 (43.038 sec) : loss = 0.18524
INFO: epoch 52, it 52753 >> 75.20 (64.350 sec) : loss = 0.25231
INFO: epoch 52, it 53000 >> 100.00 (84.523 sec) : lr 0.0327, train loss 0.25870
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.5
INFO: test : error = 3.16
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 53, it 53251 >> 25.00 (21.720 sec) : loss = 0.24292
INFO: epoch 53, it 53502 >> 50.10 (43.192 sec) : loss = 0.22001
INFO: epoch 53, it 53753 >> 75.20 (64.486 sec) : loss = 0.28612
INFO: epoch 53, it 54000 >> 100.00 (84.661 sec) : lr 0.0318, train loss 0.25790
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.5
INFO: test : error = 2.98
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 54, it 54251 >> 25.00 (21.573 sec) : loss = 0.23543
INFO: epoch 54, it 54502 >> 50.10 (42.926 sec) : loss = 0.22862
INFO: epoch 54, it 54753 >> 75.20 (64.273 sec) : loss = 0.25762
INFO: epoch 54, it 55000 >> 100.00 (84.498 sec) : lr 0.0308, train loss 0.25840
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 2.97
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 55, it 55251 >> 25.00 (21.557 sec) : loss = 0.47458
INFO: epoch 55, it 55502 >> 50.10 (42.908 sec) : loss = 0.23096
INFO: epoch 55, it 55753 >> 75.20 (64.316 sec) : loss = 0.23187
INFO: epoch 55, it 56000 >> 100.00 (84.439 sec) : lr 0.0299, train loss 0.26155
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 3.25
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 56, it 56251 >> 25.00 (21.602 sec) : loss = 0.27802
INFO: epoch 56, it 56502 >> 50.10 (42.869 sec) : loss = 0.25681
INFO: epoch 56, it 56753 >> 75.20 (64.254 sec) : loss = 0.26246
INFO: epoch 56, it 57000 >> 100.00 (84.270 sec) : lr 0.0289, train loss 0.26072
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.0
INFO: test : error = 3.21
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 57, it 57251 >> 25.00 (21.618 sec) : loss = 0.19451
INFO: epoch 57, it 57502 >> 50.10 (42.974 sec) : loss = 0.21644
INFO: epoch 57, it 57753 >> 75.20 (64.302 sec) : loss = 0.28099
INFO: epoch 57, it 58000 >> 100.00 (84.412 sec) : lr 0.0279, train loss 0.25706
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 3.12
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 58, it 58251 >> 25.00 (21.543 sec) : loss = 0.31956
INFO: epoch 58, it 58502 >> 50.10 (43.018 sec) : loss = 0.26526
INFO: epoch 58, it 58753 >> 75.20 (64.402 sec) : loss = 0.25518
INFO: epoch 58, it 59000 >> 100.00 (84.525 sec) : lr 0.0270, train loss 0.25787
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 3.01
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 59, it 59251 >> 25.00 (21.729 sec) : loss = 0.23086
INFO: epoch 59, it 59502 >> 50.10 (43.023 sec) : loss = 0.28499
INFO: epoch 59, it 59753 >> 75.20 (64.353 sec) : loss = 0.27318
INFO: epoch 59, it 60000 >> 100.00 (84.480 sec) : lr 0.0260, train loss 0.25744
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 2.79
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 60, it 60251 >> 25.00 (21.750 sec) : loss = 0.28552
INFO: epoch 60, it 60502 >> 50.10 (43.002 sec) : loss = 0.23304
INFO: epoch 60, it 60753 >> 75.20 (64.243 sec) : loss = 0.21835
INFO: epoch 60, it 61000 >> 100.00 (84.374 sec) : lr 0.0250, train loss 0.25073
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.5
INFO: test : error = 3.18
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 61, it 61251 >> 25.00 (21.666 sec) : loss = 0.27070
INFO: epoch 61, it 61502 >> 50.10 (43.105 sec) : loss = 0.25229
INFO: epoch 61, it 61753 >> 75.20 (64.533 sec) : loss = 0.19957
INFO: epoch 61, it 62000 >> 100.00 (84.767 sec) : lr 0.0240, train loss 0.24936
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 2.92
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 62, it 62251 >> 25.00 (21.664 sec) : loss = 0.22331
INFO: epoch 62, it 62502 >> 50.10 (43.028 sec) : loss = 0.22239
INFO: epoch 62, it 62753 >> 75.20 (64.420 sec) : loss = 0.25267
INFO: epoch 62, it 63000 >> 100.00 (84.632 sec) : lr 0.0230, train loss 0.24976
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.3
INFO: test : error = 3.24
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 63, it 63251 >> 25.00 (21.704 sec) : loss = 0.21973
INFO: epoch 63, it 63502 >> 50.10 (42.997 sec) : loss = 0.29771
INFO: epoch 63, it 63753 >> 75.20 (64.476 sec) : loss = 0.23898
INFO: epoch 63, it 64000 >> 100.00 (84.613 sec) : lr 0.0221, train loss 0.24964
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.6
INFO: test : error = 3.09
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 64, it 64251 >> 25.00 (21.784 sec) : loss = 0.22993
INFO: epoch 64, it 64502 >> 50.10 (43.192 sec) : loss = 0.22185
INFO: epoch 64, it 64753 >> 75.20 (64.369 sec) : loss = 0.30204
INFO: epoch 64, it 65000 >> 100.00 (84.425 sec) : lr 0.0211, train loss 0.24717
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.9
INFO: test : error = 2.93
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 65, it 65251 >> 25.00 (21.625 sec) : loss = 0.22446
INFO: epoch 65, it 65502 >> 50.10 (42.867 sec) : loss = 0.22420
INFO: epoch 65, it 65753 >> 75.20 (64.097 sec) : loss = 0.19902
INFO: epoch 65, it 66000 >> 100.00 (84.108 sec) : lr 0.0201, train loss 0.24687
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.5
INFO: test : error = 3.46
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 66, it 66251 >> 25.00 (21.514 sec) : loss = 0.22207
INFO: epoch 66, it 66502 >> 50.10 (42.734 sec) : loss = 0.23183
INFO: epoch 66, it 66753 >> 75.20 (64.190 sec) : loss = 0.22713
INFO: epoch 66, it 67000 >> 100.00 (84.350 sec) : lr 0.0192, train loss 0.24657
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 3.01
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 67, it 67251 >> 25.00 (21.543 sec) : loss = 0.19302
INFO: epoch 67, it 67502 >> 50.10 (42.769 sec) : loss = 0.22377
INFO: epoch 67, it 67753 >> 75.20 (64.010 sec) : loss = 0.26523
INFO: epoch 67, it 68000 >> 100.00 (84.028 sec) : lr 0.0182, train loss 0.24500
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 2.62
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 68, it 68251 >> 25.00 (21.546 sec) : loss = 0.22232
INFO: epoch 68, it 68502 >> 50.10 (42.776 sec) : loss = 0.20091
INFO: epoch 68, it 68753 >> 75.20 (63.991 sec) : loss = 0.22274
INFO: epoch 68, it 69000 >> 100.00 (84.002 sec) : lr 0.0173, train loss 0.24191
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 2.84
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 69, it 69251 >> 25.00 (21.498 sec) : loss = 0.20171
INFO: epoch 69, it 69502 >> 50.10 (42.722 sec) : loss = 0.20569
INFO: epoch 69, it 69753 >> 75.20 (63.992 sec) : loss = 0.19960
INFO: epoch 69, it 70000 >> 100.00 (84.047 sec) : lr 0.0163, train loss 0.23977
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 3.0
INFO: test : error = 3.29
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 70, it 70251 >> 25.00 (21.788 sec) : loss = 0.25773
INFO: epoch 70, it 70502 >> 50.10 (43.005 sec) : loss = 0.23837
INFO: epoch 70, it 70753 >> 75.20 (64.186 sec) : loss = 0.30693
INFO: epoch 70, it 71000 >> 100.00 (84.170 sec) : lr 0.0154, train loss 0.23684
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.5
INFO: test : error = 3.09
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 71, it 71251 >> 25.00 (21.514 sec) : loss = 0.21814
INFO: epoch 71, it 71502 >> 50.10 (42.733 sec) : loss = 0.21594
INFO: epoch 71, it 71753 >> 75.20 (63.974 sec) : loss = 0.24025
INFO: epoch 71, it 72000 >> 100.00 (83.972 sec) : lr 0.0145, train loss 0.23679
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 2.85
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 72, it 72251 >> 25.00 (21.547 sec) : loss = 0.21726
INFO: epoch 72, it 72502 >> 50.10 (42.814 sec) : loss = 0.24514
INFO: epoch 72, it 72753 >> 75.20 (64.050 sec) : loss = 0.38408
INFO: epoch 72, it 73000 >> 100.00 (84.060 sec) : lr 0.0137, train loss 0.23406
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 2.77
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 73, it 73251 >> 25.00 (21.521 sec) : loss = 0.17338
INFO: epoch 73, it 73502 >> 50.10 (42.722 sec) : loss = 0.24757
INFO: epoch 73, it 73753 >> 75.20 (63.976 sec) : loss = 0.17198
INFO: epoch 73, it 74000 >> 100.00 (83.963 sec) : lr 0.0128, train loss 0.23440
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 2.85
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 74, it 74251 >> 25.00 (21.478 sec) : loss = 0.21849
INFO: epoch 74, it 74502 >> 50.10 (42.692 sec) : loss = 0.19830
INFO: epoch 74, it 74753 >> 75.20 (63.915 sec) : loss = 0.23797
INFO: epoch 74, it 75000 >> 100.00 (83.907 sec) : lr 0.0119, train loss 0.23254
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 3.14
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 75, it 75251 >> 25.00 (21.519 sec) : loss = 0.19421
INFO: epoch 75, it 75502 >> 50.10 (42.722 sec) : loss = 0.24242
INFO: epoch 75, it 75753 >> 75.20 (63.928 sec) : loss = 0.17962
INFO: epoch 75, it 76000 >> 100.00 (83.907 sec) : lr 0.0111, train loss 0.22892
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 3.03
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 76, it 76251 >> 25.00 (21.468 sec) : loss = 0.23518
INFO: epoch 76, it 76502 >> 50.10 (42.695 sec) : loss = 0.21235
INFO: epoch 76, it 76753 >> 75.20 (63.914 sec) : loss = 0.23964
INFO: epoch 76, it 77000 >> 100.00 (83.911 sec) : lr 0.0103, train loss 0.22577
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.7
INFO: test : error = 3.16
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 77, it 77251 >> 25.00 (21.549 sec) : loss = 0.19949
INFO: epoch 77, it 77502 >> 50.10 (42.763 sec) : loss = 0.19600
INFO: epoch 77, it 77753 >> 75.20 (63.996 sec) : loss = 0.18430
INFO: epoch 77, it 78000 >> 100.00 (84.001 sec) : lr 0.0095, train loss 0.22613
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.3
INFO: test : error = 2.98
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 78, it 78251 >> 25.00 (21.558 sec) : loss = 0.25896
INFO: epoch 78, it 78502 >> 50.10 (42.780 sec) : loss = 0.21600
INFO: epoch 78, it 78753 >> 75.20 (63.999 sec) : loss = 0.17778
INFO: epoch 78, it 79000 >> 100.00 (84.061 sec) : lr 0.0088, train loss 0.22544
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.3
INFO: test : error = 2.76
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 79, it 79251 >> 25.00 (21.677 sec) : loss = 0.25407
INFO: epoch 79, it 79502 >> 50.10 (42.983 sec) : loss = 0.23389
INFO: epoch 79, it 79753 >> 75.20 (64.333 sec) : loss = 0.19175
INFO: epoch 79, it 80000 >> 100.00 (84.353 sec) : lr 0.0080, train loss 0.22418
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 2.68
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 80, it 80251 >> 25.00 (21.500 sec) : loss = 0.23325
INFO: epoch 80, it 80502 >> 50.10 (42.748 sec) : loss = 0.19646
INFO: epoch 80, it 80753 >> 75.20 (64.118 sec) : loss = 0.22790
INFO: epoch 80, it 81000 >> 100.00 (84.251 sec) : lr 0.0073, train loss 0.21894
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 2.94
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 81, it 81251 >> 25.00 (21.708 sec) : loss = 0.18436
INFO: epoch 81, it 81502 >> 50.10 (43.210 sec) : loss = 0.18929
INFO: epoch 81, it 81753 >> 75.20 (64.658 sec) : loss = 0.17325
INFO: epoch 81, it 82000 >> 100.00 (84.862 sec) : lr 0.0066, train loss 0.21653
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.0
INFO: test : error = 2.95
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 82, it 82251 >> 25.00 (21.773 sec) : loss = 0.25896
INFO: epoch 82, it 82502 >> 50.10 (43.198 sec) : loss = 0.29665
INFO: epoch 82, it 82753 >> 75.20 (64.640 sec) : loss = 0.19548
INFO: epoch 82, it 83000 >> 100.00 (84.924 sec) : lr 0.0060, train loss 0.21500
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 2.81
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 83, it 83251 >> 25.00 (21.772 sec) : loss = 0.19349
INFO: epoch 83, it 83502 >> 50.10 (43.248 sec) : loss = 0.14334
INFO: epoch 83, it 83753 >> 75.20 (64.618 sec) : loss = 0.19234
INFO: epoch 83, it 84000 >> 100.00 (84.796 sec) : lr 0.0054, train loss 0.21287
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 2.85
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 84, it 84251 >> 25.00 (21.676 sec) : loss = 0.30880
INFO: epoch 84, it 84502 >> 50.10 (43.096 sec) : loss = 0.23409
INFO: epoch 84, it 84753 >> 75.20 (64.573 sec) : loss = 0.18518
INFO: epoch 84, it 85000 >> 100.00 (84.767 sec) : lr 0.0048, train loss 0.21154
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.2
INFO: test : error = 3.07
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 85, it 85251 >> 25.00 (21.714 sec) : loss = 0.19652
INFO: epoch 85, it 85502 >> 50.10 (43.157 sec) : loss = 0.17603
INFO: epoch 85, it 85753 >> 75.20 (64.547 sec) : loss = 0.20061
INFO: epoch 85, it 86000 >> 100.00 (84.774 sec) : lr 0.0042, train loss 0.21026
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.9
INFO: test : error = 2.73
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 86, it 86251 >> 25.00 (21.678 sec) : loss = 0.17337
INFO: epoch 86, it 86502 >> 50.10 (43.072 sec) : loss = 0.19634
INFO: epoch 86, it 86753 >> 75.20 (64.535 sec) : loss = 0.17688
INFO: epoch 86, it 87000 >> 100.00 (84.693 sec) : lr 0.0037, train loss 0.20830
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 2.58
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 87, it 87251 >> 25.00 (21.748 sec) : loss = 0.19496
INFO: epoch 87, it 87502 >> 50.10 (43.262 sec) : loss = 0.19504
INFO: epoch 87, it 87753 >> 75.20 (64.616 sec) : loss = 0.18108
INFO: epoch 87, it 88000 >> 100.00 (84.817 sec) : lr 0.0032, train loss 0.20549
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.8
INFO: test : error = 2.57
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 88, it 88251 >> 25.00 (21.699 sec) : loss = 0.18691
INFO: epoch 88, it 88502 >> 50.10 (43.103 sec) : loss = 0.21916
INFO: epoch 88, it 88753 >> 75.20 (64.611 sec) : loss = 0.20371
INFO: epoch 88, it 89000 >> 100.00 (84.837 sec) : lr 0.0027, train loss 0.20146
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.3
INFO: test : error = 2.67
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 89, it 89251 >> 25.00 (21.726 sec) : loss = 0.17996
INFO: epoch 89, it 89502 >> 50.10 (43.140 sec) : loss = 0.18547
INFO: epoch 89, it 89753 >> 75.20 (64.535 sec) : loss = 0.25774
INFO: epoch 89, it 90000 >> 100.00 (84.805 sec) : lr 0.0023, train loss 0.20173
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.8
INFO: test : error = 2.75
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 90, it 90251 >> 25.00 (21.740 sec) : loss = 0.16084
INFO: epoch 90, it 90502 >> 50.10 (43.120 sec) : loss = 0.18345
INFO: epoch 90, it 90753 >> 75.20 (64.497 sec) : loss = 0.18788
INFO: epoch 90, it 91000 >> 100.00 (84.683 sec) : lr 0.0019, train loss 0.19999
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.5
INFO: test : error = 2.57
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 91, it 91251 >> 25.00 (21.770 sec) : loss = 0.22388
INFO: epoch 91, it 91502 >> 50.10 (43.294 sec) : loss = 0.21532
INFO: epoch 91, it 91753 >> 75.20 (64.771 sec) : loss = 0.16634
INFO: epoch 91, it 92000 >> 100.00 (85.005 sec) : lr 0.0015, train loss 0.19823
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 2.77
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 92, it 92251 >> 25.00 (21.706 sec) : loss = 0.14555
INFO: epoch 92, it 92502 >> 50.10 (43.116 sec) : loss = 0.18245
INFO: epoch 92, it 92753 >> 75.20 (64.665 sec) : loss = 0.21777
INFO: epoch 92, it 93000 >> 100.00 (84.835 sec) : lr 0.0012, train loss 0.19543
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.4
INFO: test : error = 2.66
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 93, it 93251 >> 25.00 (21.671 sec) : loss = 0.15291
INFO: epoch 93, it 93502 >> 50.10 (43.083 sec) : loss = 0.21499
INFO: epoch 93, it 93753 >> 75.20 (64.480 sec) : loss = 0.19569
INFO: epoch 93, it 94000 >> 100.00 (84.713 sec) : lr 0.0009, train loss 0.19383
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.7
INFO: test : error = 2.81
INFO: >>>>>>>>>>>>>>NEW BEST PERFORMANCE<<<<<<<<
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 94, it 94251 >> 25.00 (21.537 sec) : loss = 0.17068
INFO: epoch 94, it 94502 >> 50.10 (42.811 sec) : loss = 0.16945
INFO: epoch 94, it 94753 >> 75.20 (64.015 sec) : loss = 0.18274
INFO: epoch 94, it 95000 >> 100.00 (84.003 sec) : lr 0.0007, train loss 0.19356
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.8
INFO: test : error = 2.56
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 95, it 95251 >> 25.00 (21.495 sec) : loss = 0.20125
INFO: epoch 95, it 95502 >> 50.10 (42.808 sec) : loss = 0.17855
INFO: epoch 95, it 95753 >> 75.20 (64.014 sec) : loss = 0.16781
INFO: epoch 95, it 96000 >> 100.00 (83.989 sec) : lr 0.0005, train loss 0.19128
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.3
INFO: test : error = 2.96
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 96, it 96251 >> 25.00 (21.530 sec) : loss = 0.16573
INFO: epoch 96, it 96502 >> 50.10 (42.757 sec) : loss = 0.19692
INFO: epoch 96, it 96753 >> 75.20 (64.054 sec) : loss = 0.36254
INFO: epoch 96, it 97000 >> 100.00 (84.074 sec) : lr 0.0003, train loss 0.19120
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.3
INFO: test : error = 2.91
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 97, it 97251 >> 25.00 (21.514 sec) : loss = 0.17794
INFO: epoch 97, it 97502 >> 50.10 (42.814 sec) : loss = 0.17547
INFO: epoch 97, it 97753 >> 75.20 (64.082 sec) : loss = 0.18084
INFO: epoch 97, it 98000 >> 100.00 (84.132 sec) : lr 0.0002, train loss 0.18987
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.9
INFO: test : error = 2.73
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 98, it 98251 >> 25.00 (21.585 sec) : loss = 0.19497
INFO: epoch 98, it 98502 >> 50.10 (42.812 sec) : loss = 0.27100
INFO: epoch 98, it 98753 >> 75.20 (64.056 sec) : loss = 0.18335
INFO: epoch 98, it 99000 >> 100.00 (84.057 sec) : lr 0.0001, train loss 0.19055
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.9
INFO: test : error = 2.67
INFO: >>>>>>>>>>>>>>TRAINING<<<<<<<<<<<<<<<<<<<<
INFO: epoch 99, it 99251 >> 25.00 (21.688 sec) : loss = 0.14968
INFO: epoch 99, it 99502 >> 50.10 (43.080 sec) : loss = 0.23220
INFO: epoch 99, it 99753 >> 75.20 (64.332 sec) : loss = 0.21032
INFO: epoch 99, it 100000 >> 100.00 (84.346 sec) : lr 0.0000, train loss 0.18895
INFO: >>>>>>>>>>>>>>EVALUATING<<<<<<<<<<<<<<<<<<
INFO: dev : error = 2.1
INFO: test : error = 2.61
INFO: >>>>>>>>>>>>>>BEST PERFORMANCE, epoch 93<<<<<<<<<<<<<<<<<<
INFO: dev : error = 1.7
INFO: test : error = 2.81
